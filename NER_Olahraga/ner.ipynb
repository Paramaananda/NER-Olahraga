{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        train = f.read().split(\"\\n\\n\")\n",
    "        train = [x.split(\"\\n\") for x in train]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        sentence = train[i][0].split(\" \")\n",
    "        label = train[i][1].split(\" \")\n",
    "        if len(sentence) == len(label):\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 23:12:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 12.5MB/s]                    \n",
      "2024-11-27 23:12:53 INFO: Downloaded file to C:\\Users\\Anand\\stanza_resources\\resources.json\n",
      "2024-11-27 23:12:53 WARNING: Language id package default expects mwt, which has been added\n",
      "2024-11-27 23:12:54 INFO: Loading these models for language: id (Indonesian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | gsd        |\n",
      "| mwt       | gsd        |\n",
      "| pos       | gsd_charlm |\n",
      "==========================\n",
      "\n",
      "2024-11-27 23:12:54 INFO: Using device: cpu\n",
      "2024-11-27 23:12:54 INFO: Loading: tokenize\n",
      "2024-11-27 23:12:54 INFO: Loading: mwt\n",
      "2024-11-27 23:12:54 INFO: Loading: pos\n",
      "2024-11-27 23:12:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stanza NLP pipeline\n",
    "nlp = stanza.Pipeline('id', processors='tokenize,pos')\n",
    "\n",
    "# Data Sample\n",
    "sentences, labels = extract_text(\"TAGGED REVISI DIKIT.txt\")\n",
    "\n",
    "\n",
    "# Create Vocabulary for words, POS tags, and labels\n",
    "word2idx = defaultdict(lambda: len(word2idx))\n",
    "pos2idx = defaultdict(lambda: len(pos2idx))\n",
    "label2idx = defaultdict(lambda: len(label2idx))\n",
    "\n",
    "# Add special tokens\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "pos2idx[\"<PAD>\"] = 0\n",
    "label2idx[\"O\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess training data for initial vocabulary\n",
    "def preprocess_training_data(sentences, labels):\n",
    "    processed_data = []\n",
    "    for sentence, sent_labels in zip(sentences, labels):\n",
    "        # Use Stanza for POS tagging\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        \n",
    "        processed_words = []\n",
    "        processed_pos = []\n",
    "        processed_labels = []\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                processed_words.append(word.text)\n",
    "                processed_pos.append(word.upos)\n",
    "        \n",
    "        # Match labels to processed words (assuming same order)\n",
    "        processed_labels = sent_labels[:len(processed_words)]\n",
    "        \n",
    "        processed_data.append((processed_words, processed_pos, processed_labels))\n",
    "        \n",
    "        # Update vocabularies\n",
    "        for word in processed_words:\n",
    "            word2idx[word]\n",
    "        for pos in processed_pos:\n",
    "            pos2idx[pos]\n",
    "        for label in processed_labels:\n",
    "            label2idx[label]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Preprocess the initial training data\n",
    "processed_training_data = preprocess_training_data(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words, POS tags, and labels to indices\n",
    "input_words = [[word2idx[word] for word in sentence[0]] for sentence in processed_training_data]\n",
    "input_pos = [[pos2idx[pos] for pos in sentence[1]] for sentence in processed_training_data]\n",
    "label_data = [[label2idx[label] for label in sentence[2]] for sentence in processed_training_data]\n",
    "\n",
    "# Dynamic Padding: Use the length of the longest sentence in the data\n",
    "MAX_LEN = max([len(sentence) for sentence in input_words])\n",
    "\n",
    "# Padding the sequences\n",
    "input_words = [sentence + [0]*(MAX_LEN - len(sentence)) for sentence in input_words]\n",
    "input_pos = [pos + [0]*(MAX_LEN - len(pos)) for pos in input_pos]\n",
    "label_data = [label + [0]*(MAX_LEN - len(label)) for label in label_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, words, pos, labels):\n",
    "        self.words = words\n",
    "        self.pos = pos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.pos[idx], self.labels[idx]\n",
    "    \n",
    "# Sorting by length (this is necessary for pack_padded_sequence)\n",
    "def collate_fn(batch):\n",
    "    words, pos, labels = zip(*batch)\n",
    "    \n",
    "    # Sorting by length of the words (descending order)\n",
    "    lengths = torch.tensor([len(w) for w in words])\n",
    "    sorted_idx = torch.argsort(lengths, descending=True)\n",
    "    \n",
    "    words = [words[i] for i in sorted_idx]\n",
    "    pos = [pos[i] for i in sorted_idx]\n",
    "    labels = [labels[i] for i in sorted_idx]\n",
    "    lengths = lengths[sorted_idx]\n",
    "\n",
    "    # Padding the sequences\n",
    "    words_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(w) for w in words], batch_first=True, padding_value=0)\n",
    "    pos_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in pos], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0)\n",
    "\n",
    "    return words_padded, pos_padded, labels_padded, lengths\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = NERDataset(input_words, input_pos, label_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced NER Model with POS tag input\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, tagset_size, \n",
    "                 embedding_dim=50, pos_embedding_dim=20, hidden_dim=100, dropout=0.3):\n",
    "        super(NERModel, self).__init__()\n",
    "        # Embedding layers for words and POS tags\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embedding_dim)\n",
    "        \n",
    "        # Combine word and POS embeddings\n",
    "        combined_dim = embedding_dim + pos_embedding_dim\n",
    "        \n",
    "        # LSTM layer with dropout\n",
    "        self.lstm = nn.LSTM(combined_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, words, pos):\n",
    "        # Embed words and POS tags\n",
    "        word_emb = self.word_embedding(words)\n",
    "        pos_emb = self.pos_embedding(pos)\n",
    "        \n",
    "        # Concatenate word and POS embeddings\n",
    "        combined_emb = torch.cat((word_emb, pos_emb), dim=2)\n",
    "        \n",
    "        # LSTM and classification\n",
    "        lstm_out, _ = self.lstm(combined_emb)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "vocab_size = len(word2idx)\n",
    "pos_size = len(pos2idx)\n",
    "tagset_size = len(label2idx)\n",
    "\n",
    "# Create model\n",
    "model = NERModel(vocab_size, pos_size, tagset_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding in loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8726191520690918\n",
      "Epoch 2, Loss: 1.8213127851486206\n",
      "Epoch 3, Loss: 1.7707313299179077\n",
      "Epoch 4, Loss: 1.7205090522766113\n",
      "Epoch 5, Loss: 1.6703182458877563\n",
      "Epoch 6, Loss: 1.6198782920837402\n",
      "Epoch 7, Loss: 1.5689777135849\n",
      "Epoch 8, Loss: 1.5174819231033325\n",
      "Epoch 9, Loss: 1.4653443098068237\n",
      "Epoch 10, Loss: 1.4126147031784058\n",
      "Epoch 11, Loss: 1.3594303131103516\n",
      "Epoch 12, Loss: 1.3059930801391602\n",
      "Epoch 13, Loss: 1.2525259256362915\n",
      "Epoch 14, Loss: 1.1992220878601074\n",
      "Epoch 15, Loss: 1.1461960077285767\n",
      "Epoch 16, Loss: 1.0934590101242065\n",
      "Epoch 17, Loss: 1.0409386157989502\n",
      "Epoch 18, Loss: 0.9885411858558655\n",
      "Epoch 19, Loss: 0.9362373948097229\n",
      "Epoch 20, Loss: 0.8841332793235779\n",
      "Epoch 21, Loss: 0.8324933648109436\n",
      "Epoch 22, Loss: 0.781702995300293\n",
      "Epoch 23, Loss: 0.7321908473968506\n",
      "Epoch 24, Loss: 0.6843464374542236\n",
      "Epoch 25, Loss: 0.6384745240211487\n",
      "Epoch 26, Loss: 0.5947960019111633\n",
      "Epoch 27, Loss: 0.5534727573394775\n",
      "Epoch 28, Loss: 0.5146258473396301\n",
      "Epoch 29, Loss: 0.47832316160202026\n",
      "Epoch 30, Loss: 0.44454583525657654\n",
      "Epoch 31, Loss: 0.4131585955619812\n",
      "Epoch 32, Loss: 0.3839208483695984\n",
      "Epoch 33, Loss: 0.3565565347671509\n",
      "Epoch 34, Loss: 0.3308505117893219\n",
      "Epoch 35, Loss: 0.3067129850387573\n",
      "Epoch 36, Loss: 0.2841688394546509\n",
      "Epoch 37, Loss: 0.26328590512275696\n",
      "Epoch 38, Loss: 0.24408932030200958\n",
      "Epoch 39, Loss: 0.22651442885398865\n",
      "Epoch 40, Loss: 0.21041379868984222\n",
      "Epoch 41, Loss: 0.19560423493385315\n",
      "Epoch 42, Loss: 0.18191775679588318\n",
      "Epoch 43, Loss: 0.1692308634519577\n",
      "Epoch 44, Loss: 0.1574651300907135\n",
      "Epoch 45, Loss: 0.1465686857700348\n",
      "Epoch 46, Loss: 0.13649605214595795\n",
      "Epoch 47, Loss: 0.12719708681106567\n",
      "Epoch 48, Loss: 0.11861579865217209\n",
      "Epoch 49, Loss: 0.11069386452436447\n",
      "Epoch 50, Loss: 0.10337583720684052\n",
      "Epoch 51, Loss: 0.09661245346069336\n",
      "Epoch 52, Loss: 0.09036105126142502\n",
      "Epoch 53, Loss: 0.08458442240953445\n",
      "Epoch 54, Loss: 0.07924922555685043\n",
      "Epoch 55, Loss: 0.07432369142770767\n",
      "Epoch 56, Loss: 0.06977777183055878\n",
      "Epoch 57, Loss: 0.06558270752429962\n",
      "Epoch 58, Loss: 0.06171145290136337\n",
      "Epoch 59, Loss: 0.058138784021139145\n",
      "Epoch 60, Loss: 0.054841265082359314\n",
      "Epoch 61, Loss: 0.05179762467741966\n",
      "Epoch 62, Loss: 0.04898812621831894\n",
      "Epoch 63, Loss: 0.046394433826208115\n",
      "Epoch 64, Loss: 0.043999698013067245\n",
      "Epoch 65, Loss: 0.04178803414106369\n",
      "Epoch 66, Loss: 0.039744745939970016\n",
      "Epoch 67, Loss: 0.03785569593310356\n",
      "Epoch 68, Loss: 0.03610802814364433\n",
      "Epoch 69, Loss: 0.03448950871825218\n",
      "Epoch 70, Loss: 0.03298897668719292\n",
      "Epoch 71, Loss: 0.0315961092710495\n",
      "Epoch 72, Loss: 0.030301446095108986\n",
      "Epoch 73, Loss: 0.02909652516245842\n",
      "Epoch 74, Loss: 0.027973564341664314\n",
      "Epoch 75, Loss: 0.02692568488419056\n",
      "Epoch 76, Loss: 0.02594652771949768\n",
      "Epoch 77, Loss: 0.025030363351106644\n",
      "Epoch 78, Loss: 0.024172138422727585\n",
      "Epoch 79, Loss: 0.02336706407368183\n",
      "Epoch 80, Loss: 0.02261086367070675\n",
      "Epoch 81, Loss: 0.021899711340665817\n",
      "Epoch 82, Loss: 0.02122996188700199\n",
      "Epoch 83, Loss: 0.02059839479625225\n",
      "Epoch 84, Loss: 0.02000206895172596\n",
      "Epoch 85, Loss: 0.019438154995441437\n",
      "Epoch 86, Loss: 0.01890438236296177\n",
      "Epoch 87, Loss: 0.018398325890302658\n",
      "Epoch 88, Loss: 0.017917992547154427\n",
      "Epoch 89, Loss: 0.017461486160755157\n",
      "Epoch 90, Loss: 0.017027152702212334\n",
      "Epoch 91, Loss: 0.01661342941224575\n",
      "Epoch 92, Loss: 0.016218869015574455\n",
      "Epoch 93, Loss: 0.015842212364077568\n",
      "Epoch 94, Loss: 0.015482199378311634\n",
      "Epoch 95, Loss: 0.01513785682618618\n",
      "Epoch 96, Loss: 0.01480809785425663\n",
      "Epoch 97, Loss: 0.014492001384496689\n",
      "Epoch 98, Loss: 0.014188792556524277\n",
      "Epoch 99, Loss: 0.013897624798119068\n",
      "Epoch 100, Loss: 0.013617733493447304\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):  # for simplicity, we use 10 epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for words, pos, labels, lengths in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(words, pos)\n",
    "\n",
    "        # Flatten the outputs and labels for the loss function\n",
    "        outputs = outputs.view(-1, tagset_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute loss, for all tokens (including padding)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(sentence):\n",
    "    # Use Stanza for POS tagging\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    \n",
    "    # Extract words and POS tags\n",
    "    processed_words = []\n",
    "    processed_pos = []\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            processed_words.append(word.text)\n",
    "            processed_pos.append(word.upos)\n",
    "    \n",
    "    # Convert to indices, handling out-of-vocabulary words\n",
    "    input_words = [word2idx.get(word, word2idx[\"<PAD>\"]) for word in processed_words]\n",
    "    input_pos = [pos2idx.get(pos, pos2idx[\"<PAD>\"]) for pos in processed_pos]\n",
    "\n",
    "    print(input_pos)\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_words = input_words + [0] * (MAX_LEN - len(input_words))\n",
    "    input_pos = input_pos + [0] * (MAX_LEN - len(input_pos))\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_words = torch.tensor([input_words]).long()\n",
    "    input_pos = torch.tensor([input_pos]).long()\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_words, input_pos)\n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "\n",
    "    print(outputs)\n",
    "    \n",
    "    # Convert indices back to labels\n",
    "    predicted_labels = [list(label2idx.keys())[i] for i in predicted[0]]\n",
    "\n",
    "    # Truncate to original sentence length\n",
    "    predicted_labels = predicted_labels[:len(processed_words)]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 2, 3, 3, 7, 3, 8, 6, 7, 3, 2, 2]\n",
      "tensor([[[-0.9895, -0.3388, -0.2981,  1.4286,  0.9781, -0.5685, -0.5134],\n",
      "         [-2.1508, -1.4610, -0.6643,  5.4295,  0.6401, -1.6415, -1.2331],\n",
      "         [-2.0208, -2.5483, -1.4885,  1.2121,  6.4200, -0.6240, -1.0807],\n",
      "         [-1.9078, -2.5131, -2.0502,  0.9658,  5.2353,  0.8700, -0.9629],\n",
      "         [-1.8682, -1.8244, -2.0295,  1.3754,  3.4175,  1.8726, -0.4297],\n",
      "         [-2.2606, -1.3152, -1.6946,  2.3939,  1.8620,  1.6803,  0.0916],\n",
      "         [-2.1008, -1.2035, -1.7506,  3.2557,  1.0498,  1.0543,  0.5209],\n",
      "         [-2.0609, -0.3882, -1.9209,  3.5733,  0.4202,  0.6645,  0.3359],\n",
      "         [-1.9322, -0.5706, -1.6758,  3.9669, -0.0149,  0.0972,  0.2398],\n",
      "         [-2.2716, -0.7673, -1.9419,  4.7202,  0.3696, -0.4383, -0.1416],\n",
      "         [-2.2754, -0.6196, -1.6878,  5.0830,  0.8944, -1.1121, -0.7300],\n",
      "         [-2.5820, -1.6361, -1.2808,  6.5665,  0.9111, -2.2195, -1.2715],\n",
      "         [-2.0463, -2.5599, -1.9928,  1.2388,  6.8753, -0.5562, -0.9090]]])\n",
      "Predicted Labels: ['B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG']\n",
      "\n",
      "Word to Index: {'<PAD>': 0, 'ï»': 1, '¿Luis': 2, 'Enrique': 3, 'tahu': 4, 'start': 5, 'Paris': 6, 'Saint': 7, 'Germain': 8, 'di': 9, 'Liga': 10, 'Champions': 11, 'tidak': 12, 'bagus': 13, 'Maka': 14, 'Les': 15, 'Parisiens': 16, 'dituntut': 17, 'bangkit': 18, 'meski': 19, 'punya': 20, 'catatan': 21, 'negatif': 22, 'saat': 23, 'bertemu': 24, 'Bayern': 25, 'Munich': 26}\n",
      "\n",
      "POS to Index: {'<PAD>': 0, 'PUNCT': 1, 'PROPN': 2, 'VERB': 3, 'ADP': 4, 'PART': 5, 'ADJ': 6, 'SCONJ': 7, 'NOUN': 8}\n",
      "\n",
      "Label to Index: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LEAGUE': 5, 'I-LEAGUE': 6}\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_sentence = [\"Maka\", \"Les\", \"Parisiens\", \"dituntut\", \"bangkit\", \"meski\", \"punya\", \"catatan\", \"negatif\", \"saat\", \"bertemu\", \"Bayern\", \"Munich\"]\n",
    "predictions = predict(test_sentence)\n",
    "print(\"Predicted Labels:\", predictions)\n",
    "\n",
    "# Print out vocabularies for reference\n",
    "print(\"\\nWord to Index:\", dict(word2idx))\n",
    "print(\"\\nPOS to Index:\", dict(pos2idx))\n",
    "print(\"\\nLabel to Index:\", dict(label2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 23:12:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 12.7MB/s]                    \n",
      "2024-11-27 23:12:56 INFO: Downloaded file to C:\\Users\\Anand\\stanza_resources\\resources.json\n",
      "2024-11-27 23:12:56 WARNING: Language id package default expects mwt, which has been added\n",
      "2024-11-27 23:12:56 INFO: Loading these models for language: id (Indonesian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | gsd        |\n",
      "| mwt       | gsd        |\n",
      "| pos       | gsd_charlm |\n",
      "==========================\n",
      "\n",
      "2024-11-27 23:12:56 INFO: Using device: cpu\n",
      "2024-11-27 23:12:56 INFO: Loading: tokenize\n",
      "2024-11-27 23:12:56 INFO: Loading: mwt\n",
      "2024-11-27 23:12:56 INFO: Loading: pos\n",
      "2024-11-27 23:12:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿Luis Enrique tahu start Paris Saint Germain di Liga Champions tidak bagus', 'B-PER I-PER O O B-ORG I-ORG I-ORG O B-LEAGUE I-LEAGUE O O']\n",
      "['Maka Les Parisiens dituntut bangkit meski punya catatan negatif saat bertemu Bayern Munich', 'O B-ORG I-ORG O O O O O O O O B-ORG I-ORG']\n",
      "[['ï»¿Luis', 'Enrique', 'tahu', 'start', 'Paris', 'Saint', 'Germain', 'di', 'Liga', 'Champions', 'tidak', 'bagus'], ['Maka', 'Les', 'Parisiens', 'dituntut', 'bangkit', 'meski', 'punya', 'catatan', 'negatif', 'saat', 'bertemu', 'Bayern', 'Munich']]\n",
      "Epoch 1, Loss: 1.9105525016784668\n",
      "Epoch 2, Loss: 1.831746220588684\n",
      "Epoch 3, Loss: 1.755952000617981\n",
      "Epoch 4, Loss: 1.6827203035354614\n",
      "Epoch 5, Loss: 1.6122287511825562\n",
      "Epoch 6, Loss: 1.545040488243103\n",
      "Epoch 7, Loss: 1.4817801713943481\n",
      "Epoch 8, Loss: 1.4228688478469849\n",
      "Epoch 9, Loss: 1.3683631420135498\n",
      "Epoch 10, Loss: 1.317850112915039\n",
      "Epoch 11, Loss: 1.2704427242279053\n",
      "Epoch 12, Loss: 1.2249923944473267\n",
      "Epoch 13, Loss: 1.1804107427597046\n",
      "Epoch 14, Loss: 1.1359336376190186\n",
      "Epoch 15, Loss: 1.091251015663147\n",
      "Epoch 16, Loss: 1.0464708805084229\n",
      "Epoch 17, Loss: 1.001930594444275\n",
      "Epoch 18, Loss: 0.957952082157135\n",
      "Epoch 19, Loss: 0.9146612286567688\n",
      "Epoch 20, Loss: 0.8719357848167419\n",
      "Epoch 21, Loss: 0.8294713497161865\n",
      "Epoch 22, Loss: 0.7869325280189514\n",
      "Epoch 23, Loss: 0.7441453337669373\n",
      "Epoch 24, Loss: 0.7012695074081421\n",
      "Epoch 25, Loss: 0.6588321924209595\n",
      "Epoch 26, Loss: 0.617516279220581\n",
      "Epoch 27, Loss: 0.5778241753578186\n",
      "Epoch 28, Loss: 0.5399389266967773\n",
      "Epoch 29, Loss: 0.5039251446723938\n",
      "Epoch 30, Loss: 0.46992766857147217\n",
      "Epoch 31, Loss: 0.43813517689704895\n",
      "Epoch 32, Loss: 0.40858280658721924\n",
      "Epoch 33, Loss: 0.3810613751411438\n",
      "Epoch 34, Loss: 0.3552747964859009\n",
      "Epoch 35, Loss: 0.3310091495513916\n",
      "Epoch 36, Loss: 0.3081624209880829\n",
      "Epoch 37, Loss: 0.28669700026512146\n",
      "Epoch 38, Loss: 0.26659613847732544\n",
      "Epoch 39, Loss: 0.24785448610782623\n",
      "Epoch 40, Loss: 0.23047159612178802\n",
      "Epoch 41, Loss: 0.21442590653896332\n",
      "Epoch 42, Loss: 0.1996527761220932\n",
      "Epoch 43, Loss: 0.1860504150390625\n",
      "Epoch 44, Loss: 0.17350545525550842\n",
      "Epoch 45, Loss: 0.16191287338733673\n",
      "Epoch 46, Loss: 0.15118390321731567\n",
      "Epoch 47, Loss: 0.1412467062473297\n",
      "Epoch 48, Loss: 0.13204491138458252\n",
      "Epoch 49, Loss: 0.1235361099243164\n",
      "Epoch 50, Loss: 0.1156887337565422\n",
      "Epoch 51, Loss: 0.10847439616918564\n",
      "Epoch 52, Loss: 0.1018577516078949\n",
      "Epoch 53, Loss: 0.09579107165336609\n",
      "Epoch 54, Loss: 0.09021717309951782\n",
      "Epoch 55, Loss: 0.08507824689149857\n",
      "Epoch 56, Loss: 0.08032295852899551\n",
      "Epoch 57, Loss: 0.07591016590595245\n",
      "Epoch 58, Loss: 0.07180812954902649\n",
      "Epoch 59, Loss: 0.06799257546663284\n",
      "Epoch 60, Loss: 0.06444425880908966\n",
      "Epoch 61, Loss: 0.06114725023508072\n",
      "Epoch 62, Loss: 0.05808740481734276\n",
      "Epoch 63, Loss: 0.055251337587833405\n",
      "Epoch 64, Loss: 0.052625205367803574\n",
      "Epoch 65, Loss: 0.050194285809993744\n",
      "Epoch 66, Loss: 0.047942813485860825\n",
      "Epoch 67, Loss: 0.04585469886660576\n",
      "Epoch 68, Loss: 0.04391422122716904\n",
      "Epoch 69, Loss: 0.04210641607642174\n",
      "Epoch 70, Loss: 0.04041758552193642\n",
      "Epoch 71, Loss: 0.0388355627655983\n",
      "Epoch 72, Loss: 0.03734948858618736\n",
      "Epoch 73, Loss: 0.03595014661550522\n",
      "Epoch 74, Loss: 0.03462987020611763\n",
      "Epoch 75, Loss: 0.03338231146335602\n",
      "Epoch 76, Loss: 0.03220248967409134\n",
      "Epoch 77, Loss: 0.03108661249279976\n",
      "Epoch 78, Loss: 0.030031859874725342\n",
      "Epoch 79, Loss: 0.029036052525043488\n",
      "Epoch 80, Loss: 0.028097044676542282\n",
      "Epoch 81, Loss: 0.02721233479678631\n",
      "Epoch 82, Loss: 0.02637898363173008\n",
      "Epoch 83, Loss: 0.02559337019920349\n",
      "Epoch 84, Loss: 0.024851877242326736\n",
      "Epoch 85, Loss: 0.024150632321834564\n",
      "Epoch 86, Loss: 0.02348628081381321\n",
      "Epoch 87, Loss: 0.022855572402477264\n",
      "Epoch 88, Loss: 0.022255538031458855\n",
      "Epoch 89, Loss: 0.02168387360870838\n",
      "Epoch 90, Loss: 0.021138256415724754\n",
      "Epoch 91, Loss: 0.020616956055164337\n",
      "Epoch 92, Loss: 0.020118318498134613\n",
      "Epoch 93, Loss: 0.019641024991869926\n",
      "Epoch 94, Loss: 0.019183889031410217\n",
      "Epoch 95, Loss: 0.01874568685889244\n",
      "Epoch 96, Loss: 0.018325500190258026\n",
      "Epoch 97, Loss: 0.017922328785061836\n",
      "Epoch 98, Loss: 0.017535291612148285\n",
      "Epoch 99, Loss: 0.017163464799523354\n",
      "Epoch 100, Loss: 0.016806112602353096\n",
      "Predicted Labels: ['B-ORG', 'B-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG']\n",
      "\n",
      "POS to Index: {'<PAD>': 0, 'PUNCT': 1, 'PROPN': 2, 'VERB': 3, 'ADP': 4, 'PART': 5, 'ADJ': 6, 'SCONJ': 7, 'NOUN': 8}\n",
      "\n",
      "Label to Index: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LEAGUE': 5, 'I-LEAGUE': 6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import stanza\n",
    "\n",
    "def extract_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        train = f.read().split(\"\\n\\n\")\n",
    "        train = [x.split(\"\\n\") for x in train]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        print(train[i])\n",
    "        sentence = train[i][0].split(\" \")\n",
    "        label = train[i][1].split(\" \")\n",
    "        if len(sentence) == len(label):\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Initialize Stanza NLP pipeline\n",
    "nlp = stanza.Pipeline('id', processors='tokenize,pos')\n",
    "\n",
    "# Data Sample\n",
    "sentences, labels = extract_text(\"TAGGED REVISI DIKIT.txt\")\n",
    "print(sentences)\n",
    "\n",
    "# Create Vocabulary for POS tags and labels\n",
    "pos2idx = defaultdict(lambda: len(pos2idx))\n",
    "label2idx = defaultdict(lambda: len(label2idx))\n",
    "\n",
    "# Add special tokens\n",
    "pos2idx[\"<PAD>\"] = 0\n",
    "label2idx[\"O\"] = 0\n",
    "\n",
    "# Preprocess training data for initial vocabulary\n",
    "def preprocess_training_data(sentences, labels):\n",
    "    processed_data = []\n",
    "    for sentence, sent_labels in zip(sentences, labels):\n",
    "        # Use Stanza for POS tagging\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        \n",
    "        processed_pos = []\n",
    "        processed_labels = []\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                processed_pos.append(word.upos)\n",
    "        \n",
    "        # Match labels to processed POS tags (assuming same order)\n",
    "        processed_labels = sent_labels[:len(processed_pos)]\n",
    "        \n",
    "        processed_data.append((processed_pos, processed_labels))\n",
    "        \n",
    "        # Update vocabularies\n",
    "        for pos in processed_pos:\n",
    "            pos2idx[pos]\n",
    "        for label in processed_labels:\n",
    "            label2idx[label]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Preprocess the initial training data\n",
    "processed_training_data = preprocess_training_data(sentences, labels)\n",
    "\n",
    "# Convert POS tags and labels to indices\n",
    "input_pos = [[pos2idx[pos] for pos in sentence[0]] for sentence in processed_training_data]\n",
    "label_data = [[label2idx[label] for label in sentence[1]] for sentence in processed_training_data]\n",
    "\n",
    "# Dynamic Padding: Use the length of the longest sentence in the data\n",
    "MAX_LEN = max([len(sentence) for sentence in input_pos])\n",
    "\n",
    "# Padding the sequences\n",
    "input_pos = [pos + [0]*(MAX_LEN - len(pos)) for pos in input_pos]\n",
    "label_data = [label + [0]*(MAX_LEN - len(label)) for label in label_data]\n",
    "\n",
    "# Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, pos, labels):\n",
    "        self.pos = pos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos[idx], self.labels[idx]\n",
    "    \n",
    "# Sorting by length (this is necessary for pack_padded_sequence)\n",
    "def collate_fn(batch):\n",
    "    pos, labels = zip(*batch)\n",
    "    \n",
    "    # Sorting by length of the POS tags (descending order)\n",
    "    lengths = torch.tensor([len(p) for p in pos])\n",
    "    sorted_idx = torch.argsort(lengths, descending=True)\n",
    "    \n",
    "    pos = [pos[i] for i in sorted_idx]\n",
    "    labels = [labels[i] for i in sorted_idx]\n",
    "    lengths = lengths[sorted_idx]\n",
    "\n",
    "    # Padding the sequences\n",
    "    pos_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in pos], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0)\n",
    "\n",
    "    return pos_padded, labels_padded, lengths\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = NERDataset(input_pos, label_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Enhanced NER Model with POS tag input\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, pos_size, tagset_size, pos_embedding_dim=100, hidden_dim=100, dropout=0):\n",
    "        super(NERModel, self).__init__()\n",
    "        # Embedding layer for POS tags\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embedding_dim)\n",
    "        \n",
    "        # LSTM layer with dropout\n",
    "        self.lstm = nn.LSTM(pos_embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        # Embed POS tags\n",
    "        pos_emb = self.pos_embedding(pos)\n",
    "        \n",
    "        # LSTM and classification\n",
    "        lstm_out, _ = self.lstm(pos_emb)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Set model parameters\n",
    "pos_size = len(pos2idx)\n",
    "tagset_size = len(label2idx)\n",
    "\n",
    "# Create model\n",
    "model = NERModel(pos_size, tagset_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding in loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # for simplicity, we use 100 epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for pos, labels, lengths in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(pos)\n",
    "\n",
    "        # Flatten the outputs and labels for the loss function\n",
    "        outputs = outputs.view(-1, tagset_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Create mask to ignore padding tokens in the loss calculation\n",
    "        mask = labels != 0  # Only valid tokens, ignore padding (0)\n",
    "\n",
    "        # Check if there are any valid tokens to compute loss\n",
    "        if mask.sum() == 0:  # No valid tokens to calculate loss\n",
    "            continue\n",
    "        \n",
    "        # Apply the mask to the outputs and labels (flattened)\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        # Compute loss, only for valid tokens (non-padding)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# Predict function (using only POS tags as input)\n",
    "def predict(sentence):\n",
    "    # Use Stanza for POS tagging\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    \n",
    "    # Extract POS tags\n",
    "    processed_pos = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    # Convert to indices, handling out-of-vocabulary POS tags\n",
    "    input_pos = [pos2idx.get(pos, pos2idx[\"<PAD>\"]) for pos in processed_pos]\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_pos = input_pos + [0] * (MAX_LEN - len(input_pos))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_pos = torch.tensor([input_pos]).long()\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_pos)\n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "\n",
    "    # Convert indices back to labels\n",
    "    predicted_labels = [list(label2idx.keys())[i] for i in predicted[0]]\n",
    "\n",
    "    # Truncate to original sentence length\n",
    "    predicted_labels = predicted_labels[:len(processed_pos)]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "# Test the model\n",
    "test_sentence = [\"Maka\", \"Les\", \"Parisiens\", \"dituntut\", \"bangkit\", \"meski\", \"punya\", \"catatan\", \"negatif\", \"saat\", \"bertemu\", \"Bayern\", \"Munich\"]\n",
    "predictions = predict(test_sentence)\n",
    "print(\"Predicted Labels:\", predictions)\n",
    "\n",
    "# Print out vocabularies for reference\n",
    "print(\"\\nPOS to Index:\", dict(pos2idx))\n",
    "print(\"\\nLabel to Index:\", dict(label2idx))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
